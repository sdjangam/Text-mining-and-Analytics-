{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-gram Tutorial (Python 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building N-Gram models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "724725 , ï»¿The Project Gutenberg EBook of Pride and Prejud ...\n"
     ]
    }
   ],
   "source": [
    "# Load text from Project Gutenberg URL\n",
    "import urllib2\n",
    "#f = urllib2.urlopen(\"http://www.gutenberg.org/files/1342/1342-0.txt\", 'r')\n",
    "#txt = f.read()\n",
    "#f.close()\n",
    "txt=\"\"\n",
    "with open(\"/mnt/c/Users/gmanish/Dropbox/latest/openminds/slides/TextMining/n-gram models/1342-0.txt\") as f:\n",
    "    content = f.readlines()\n",
    "for c in content:\n",
    "    txt+=c\n",
    "\n",
    "print len(txt), ',', txt[:50] , '...'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, now lets split into words into a big list, splitting on anything non-alphanumeric [A-Za-z0-9] (as well as punctuation) and forcing everything lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125901\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "words = re.split('[^A-Za-z]+', txt.lower())\n",
    "words = filter(None, words) # Remove empty strings\n",
    "# Print length of list\n",
    "print(len(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sets\n",
    "From this we can now generate N-grams, lets start with a 1-gram, basically the set of all the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6530\n",
      "['foul', 'four', 'woods', 'hanging', 'woody', 'looking', 'eligible', 'scold', 'lord', 'meadows', 'sinking', 'leisurely', 'bringing', 'disturb', 'recollections', 'wednesday', 'piling', 'persisted', 'succession', 'tired']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:1: DeprecationWarning: the sets module is deprecated\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "import sets\n",
    "# Create set of all unique words, this throws away any information about frequency however\n",
    "gram1 = set(words)\n",
    "print len(gram1)\n",
    "# Instead of printing all the elements in the set, create an iterator and print 20 elements only\n",
    "gram1_iter = iter(gram1)\n",
    "print [gram1_iter.next() for i in xrange(20)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets try and get the 2-gram now, which is pairs of words. Let's have a quick look to see the last 10 and how they look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subscribe to\n",
      "to our\n",
      "our email\n",
      "email newsletter\n",
      "newsletter to\n",
      "to hear\n",
      "hear about\n",
      "about new\n",
      "new ebooks\n"
     ]
    }
   ],
   "source": [
    "# See the last 10 pairs\n",
    "for i in xrange(len(words)-10, len(words)-1):\n",
    "    print words[i], words[i+1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, seems good, lets get all word pairs, and then generate a set of unique pairs from it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125900\n",
      "55640\n",
      "[('her', 'taste'), ('every', 'kind'), ('best', 'friend'), ('soothed', 'but'), ('seemed', 'most'), ('fortune', 'it'), ('of', 'thanking'), ('near', 'she'), ('understand', 'from'), ('it', 'looks'), ('have', 'made'), ('lucas', 'he'), ('fail', 'him'), ('new', 'to'), ('nothing', 'but'), ('fearful', 'on'), ('to', 'wander'), ('write', 'rather'), ('of', 'studying'), ('interruption', 'from')]\n"
     ]
    }
   ],
   "source": [
    "word_pairs = [(words[i], words[i+1]) for i in xrange(len(words)-1)]\n",
    "print len(word_pairs)\n",
    "gram2 = set(word_pairs)\n",
    "print len(gram2)\n",
    "# Print 20 elements from gram2\n",
    "gram2_iter = iter(gram2)\n",
    "print [gram2_iter.next() for i in xrange(20)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency\n",
    "\n",
    "Okay, that was fun, but this isn't enough, we need frequency if we want to have any sense of probabilities, which is what N-grams are about. Instead of using sets, lets create a dictionary with counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 4507), ('to', 4243), ('of', 3730), ('and', 3658), ('her', 2225), ('i', 2070), ('a', 2012), ('in', 1937), ('was', 1847), ('she', 1710), ('that', 1594), ('it', 1550), ('not', 1450), ('you', 1428), ('he', 1339), ('his', 1271), ('be', 1260), ('as', 1192), ('had', 1177), ('with', 1100)]\n"
     ]
    }
   ],
   "source": [
    "gram1 = dict()\n",
    "\n",
    "# Populate 1-gram dictionary\n",
    "for word in words:\n",
    "    if gram1.has_key(word):\n",
    "        gram1[word] += 1\n",
    "    else:\n",
    "        gram1[word] = 1 # Start a new entry with 1 count since saw it for the first time.\n",
    "\n",
    "# Turn into a list of (word, count) sorted by count from most to least\n",
    "gram1 = sorted(gram1.items(), key=lambda (word, count): -count)\n",
    "\n",
    "# Print top 20 most frequent words\n",
    "print gram1[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Pride and Prejudice, the words 'the', 'to', 'of', and 'and' were the top four most common words. Sounds about right, not too interesting yet, lets see what happens with 2-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('of', 'the'), 491), (('to', 'be'), 445), (('in', 'the'), 397), (('i', 'am'), 303), (('mr', 'darcy'), 273), (('to', 'the'), 268), (('of', 'her'), 261), (('it', 'was'), 251), (('of', 'his'), 235), (('she', 'was'), 212), (('she', 'had'), 205), (('had', 'been'), 200), (('it', 'is'), 194), (('i', 'have'), 188), (('to', 'her'), 179), (('that', 'he'), 177), (('could', 'not'), 167), (('he', 'had'), 166), (('and', 'the'), 165), (('for', 'the'), 163)]\n"
     ]
    }
   ],
   "source": [
    "gram2 = dict()\n",
    "\n",
    "# Populate 2-gram dictionary\n",
    "for i in xrange(len(words)-1):\n",
    "    key = (words[i], words[i+1])\n",
    "    if gram2.has_key(key):\n",
    "        gram2[key] += 1\n",
    "    else:\n",
    "        gram2[key] = 1\n",
    "\n",
    "# Turn into a list of (word, count) sorted by count from most to least\n",
    "gram2 = sorted(gram2.items(), key=lambda (_, count): -count)\n",
    "\n",
    "# Print top 20 most frequent words\n",
    "print gram2[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like `\"of the\"` and `\"to be\"` are the top two most common 2-grams, sounds good. \n",
    "\n",
    "## Next word prediction using 2-gram models (max prob)\n",
    "What can we do with this? Well lets see what happens if we take a random word from all the words, and build a sentence by just choosing the most common pair that has that word as it's start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delighted\n"
     ]
    }
   ],
   "source": [
    "start_word = words[len(words)/4] #just took some random word\n",
    "print start_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now in a loop, iterate through the frequency list (most frequent first) and see if it matches the first word in a pair, if so, the next word is the second element in the word pair, and continue with that word. Stop after N words or the list does not contain that word.\n",
    "\n",
    "    *Note* : gram2 is a list that contains (key,value) where key is a word pair (first, second),\n",
    "             so you need element[0][0] for first word and element [0][1] for second word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start word: delighted\n",
      "2-gram sentence: \" delighted with the whole of the whole of the whole of the whole of the whole of the whole of \"\n"
     ]
    }
   ],
   "source": [
    "def get2GramSentence(word, n = 50):\n",
    "    for i in xrange(n):\n",
    "        print word,\n",
    "        # Find Next word\n",
    "        word = next((element[0][1] for element in gram2 if element[0][0] == word), None)\n",
    "        if not word:\n",
    "            break\n",
    "\n",
    "word = start_word\n",
    "print \"Start word: %s\" % word\n",
    "\n",
    "print \"2-gram sentence: \\\"\",\n",
    "get2GramSentence(word, 20)\n",
    "print \"\\\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It gets stuck in a loop pretty much straight away. Not very interesting, try out other words and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start word: and\n",
      "2-gram sentence: \" and the whole of the whole of the whole of the whole of the whole of the whole of the \"\n",
      "Start word: he\n",
      "2-gram sentence: \" he had been so much as to be so much as to be so much as to be so much \"\n",
      "Start word: she\n",
      "2-gram sentence: \" she was not be so much as to be so much as to be so much as to be so \"\n",
      "Start word: when\n",
      "2-gram sentence: \" when she was not be so much as to be so much as to be so much as to be \"\n",
      "Start word: john\n",
      "2-gram sentence: \" john with the whole of the whole of the whole of the whole of the whole of the whole of \"\n",
      "Start word: never\n",
      "2-gram sentence: \" never be so much as to be so much as to be so much as to be so much as \"\n",
      "Start word: i\n",
      "2-gram sentence: \" i am sure i am sure i am sure i am sure i am sure i am sure i am \"\n",
      "Start word: how\n",
      "2-gram sentence: \" how much as to be so much as to be so much as to be so much as to be \"\n"
     ]
    }
   ],
   "source": [
    "for word in ['and', 'he', 'she', 'when', 'john', 'never', 'i', 'how']:\n",
    "    print \"Start word: %s\" % word\n",
    "\n",
    "    print \"2-gram sentence: \\\"\",\n",
    "    get2GramSentence(word, 20)\n",
    "    print \"\\\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next word prediction using 2-gram models (Weighted random choice based on freq)\n",
    "Lets randomly choose from the subset of all 2grams that matches the first word, using a weighted-probability based on counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "def weighted_choice(choices):\n",
    "   total = sum(w for c, w in choices)\n",
    "   r = random.uniform(0, total)\n",
    "   upto = 0\n",
    "   for c, w in choices:\n",
    "      if upto + w > r:\n",
    "         return c\n",
    "      upto += w\n",
    "    \n",
    "def get2GramSentenceRandom(word, n = 50):\n",
    "    for i in xrange(n):\n",
    "        print word,\n",
    "        # Get all possible elements ((first word, second word), frequency)\n",
    "        choices = [element for element in gram2 if element[0][0] == word]\n",
    "        if not choices:\n",
    "            break\n",
    "        \n",
    "        # Choose a pair with weighted probability from the choice list\n",
    "        word = weighted_choice(choices)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start word: and\n",
      "2-gram sentence: \" and by venturing to lady i moved mr collins as a carriage drove along the way i were the matter \"\n",
      "Start word: he\n",
      "2-gram sentence: \" he could your education can i instantly to the colour there were but every attempt little inconvenience to conceal your \"\n",
      "Start word: she\n",
      "2-gram sentence: \" she was in his sister and was able to make mr darcy just as your respected mother s being useful \"\n",
      "Start word: when\n",
      "2-gram sentence: \" when i had she had been too much that they had been as she had done as i always coincide \"\n",
      "Start word: john\n",
      "2-gram sentence: \" john told what a clergyman i believe in the party distributing project gutenberg tm license included jane in what are \"\n",
      "Start word: never\n",
      "2-gram sentence: \" never attended her a reconciliation with them do not a liking which miss de bourgh only one most humiliating picture \"\n",
      "Start word: i\n",
      "2-gram sentence: \" i am sure i would most willingly have been so soon as wickham as it in time when the greater \"\n",
      "Start word: how\n",
      "2-gram sentence: \" how acutely did experience of regard or admit his account that can summon more of their duty to the road \"\n"
     ]
    }
   ],
   "source": [
    "for word in ['and', 'he', 'she', 'when', 'john', 'never', 'i', 'how']:\n",
    "    print \"Start word: %s\" % word\n",
    "\n",
    "    print \"2-gram sentence: \\\"\",\n",
    "    get2GramSentenceRandom(word, 20)\n",
    "    print \"\\\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those are starting to look like sentences!\n",
    "\n",
    "\n",
    "Let's try a longer sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start word: it\n",
      "2-gram sentence: \" it was marked her triumph very pretty and folly indeed if elizabeth will approve of elizabeth some time i never be levelled at longbourn on her colour but as soon grow afraid he would not likely to whisper for she tried to the longbourn a smile could not long seated \"\n"
     ]
    }
   ],
   "source": [
    "word = 'it'\n",
    "print \"Start word: %s\" % word\n",
    "print \"2-gram sentence: \\\"\",\n",
    "get2GramSentenceRandom(word, 50)\n",
    "print \"\\\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty cool, lets see what happens when we go to N-grams above 2.\n",
    "## Creating Tri-grams and higher n-gram models\n",
    "Okay, let's create a Ngram generator that can let us make ngrams of arbitrary sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateUpToNgram(n=1):\n",
    "    grams=dict()\n",
    "    for j in xrange(1,n+1):\n",
    "        gram=dict()\n",
    "        for i in xrange(len(words)-(j-1)):\n",
    "            if j==1: #unigrams\n",
    "                #gram is a dict of word to count\n",
    "                key = tuple(words[i:i+1])\n",
    "                if gram.has_key(key):\n",
    "                    gram[key] += 1\n",
    "                else:\n",
    "                    gram[key] = 1\n",
    "            else: #n-grams with n>1\n",
    "                #gram is a dict of n-1 sized gram to (dict of nth word to freq)\n",
    "                key = tuple(words[i:i+j-1])\n",
    "                localDict=dict()\n",
    "                if gram.has_key(key):\n",
    "                    localDict=gram[key]\n",
    "                    if localDict.has_key(words[i+j-1]):\n",
    "                        localDict[words[i+j-1]]+=1\n",
    "                    else:\n",
    "                        localDict[words[i+j-1]]=1\n",
    "                else:\n",
    "                    localDict[words[i+j-1]]=1\n",
    "                gram[key] = localDict\n",
    "        if j==1:\n",
    "            gram = sorted(gram.items(), key=lambda (_, count): -count)\n",
    "        if j!=1:\n",
    "            for x in gram:\n",
    "                gram[x] = sorted(gram[x].items(), key=lambda (_, count): -count)\n",
    "        grams[j]=gram\n",
    "    return grams\n",
    "\n",
    "ngrams= generateUpToNgram(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "(('the',), 4507)\n",
      "(('to',), 4243)\n",
      "(('of',), 3730)\n",
      "(('and',), 3658)\n",
      "(('her',), 2225)\n",
      "(('i',), 2070)\n",
      "(('a',), 2012)\n",
      "(('in',), 1937)\n",
      "(('was',), 1847)\n",
      "(('she',), 1710)\n",
      "(('that',), 1594)\n",
      "2\n",
      "('resolve',) [('very', 1), ('to', 1)]\n",
      "('waited',) [('on', 3), ('for', 2), ('only', 1), ('at', 1)]\n",
      "('housekeeping',) [('i', 1), ('when', 1), ('her', 1)]\n",
      "('defective',) [('the', 1), ('work', 1), ('or', 1), ('you', 1)]\n",
      "('intricate',) [('character', 1), ('characters', 1)]\n",
      "('wavering',) [('before', 1)]\n",
      "('now',) [('i', 9), ('and', 8), ('to', 6), ('be', 6), ('have', 5), ('as', 4), ('when', 4), ('a', 4), ('the', 4), ('she', 3), ('come', 3), ('made', 3), ('in', 3), ('indeed', 2), ('only', 2), ('do', 2), ('because', 2), ('absolutely', 2), ('said', 2), ('for', 2), ('told', 2), ('we', 2), ('put', 2), ('on', 2), ('of', 2), ('fast', 2), ('been', 2), ('passed', 2), ('was', 2), ('but', 2), ('convinced', 2), ('determined', 2), ('it', 2), ('at', 2), ('saw', 2), ('began', 2), ('so', 2), ('despise', 1), ('being', 1), ('openly', 1), ('quartered', 1), ('now', 1), ('thank', 1), ('except', 1), ('georgiana', 1), ('going', 1), ('struck', 1), ('wished', 1), ('seeking', 1), ('very', 1), ('kitty', 1), ('mention', 1), ('suffering', 1), ('nearly', 1), ('they', 1), ('fall', 1), ('accepted', 1), ('recollected', 1), ('necessary', 1), ('heightened', 1), ('lost', 1), ('doubled', 1), ('anxious', 1), ('become', 1), ('approaching', 1), ('some', 1), ('feel', 1), ('are', 1), ('escape', 1), ('employment', 1), ('provided', 1), ('what', 1), ('learnt', 1), ('appear', 1), ('seriously', 1), ('free', 1), ('let', 1), ('here', 1), ('teach', 1), ('painfully', 1), ('obtained', 1), ('about', 1), ('asked', 1), ('first', 1), ('appeared', 1), ('brought', 1), ('rendering', 1), ('done', 1), ('lizzy', 1), ('given', 1), ('rushed', 1), ('there', 1), ('add', 1), ('much', 1), ('expected', 1), ('entered', 1), ('tell', 1), ('more', 1), ('enough', 1), ('relieved', 1), ('occurred', 1), ('applied', 1), ('say', 1), ('seated', 1), ('near', 1), ('suppose', 1), ('mr', 1), ('my', 1), ('called', 1), ('gone', 1), ('give', 1), ('almost', 1), ('is', 1), ('an', 1), ('high', 1), ('if', 1), ('forced', 1), ('productive', 1), ('perhaps', 1), ('that', 1), ('concealed', 1), ('take', 1), ('you', 1), ('several', 1), ('added', 1), ('elizabeth', 1), ('living', 1), ('most', 1), ('smiled', 1), ('nothing', 1), ('walked', 1), ('complaining', 1), ('dissolved', 1), ('having', 1), ('came', 1), ('know', 1)]\n",
      "('thwarted',) [('so', 1)]\n",
      "('weighed',) [('on', 1), ('every', 1)]\n",
      "('rate',) [('a', 1), ('you', 1), ('there', 1), ('was', 1), ('she', 1)]\n",
      "('struggle',) [('in', 1)]\n",
      "3\n",
      "('her', 'taste') [('and', 1), ('is', 1)]\n",
      "('every', 'kind') [('of', 1)]\n",
      "('best', 'friend') [('could', 1), ('it', 1)]\n",
      "('soothed', 'but') [('it', 1)]\n",
      "('seemed', 'most') [('improbable', 1)]\n",
      "('fortune', 'it') [('is', 1)]\n",
      "('of', 'thanking') [('you', 1), ('her', 1)]\n",
      "('near', 'she') [('would', 1)]\n",
      "('understand', 'from') [('whence', 1), ('mrs', 1)]\n",
      "('it', 'looks') [('just', 1)]\n",
      "('have', 'made') [('no', 2), ('him', 2), ('a', 1), ('long', 1), ('such', 1), ('the', 1)]\n",
      "4\n",
      "('at', 'the', 'beginning') [('of', 1)]\n",
      "('pause', 'followed', 'this') [('speech', 1)]\n",
      "('catherine', 'had', 'many') [('other', 1)]\n",
      "('solicit', 'for', 'the') [('honour', 1)]\n",
      "('finished', 'can', 'it') [('be', 1)]\n",
      "('that', 'is', 'very') [('true', 2), ('unlucky', 1), ('pleasing', 1), ('decided', 1), ('strange', 1)]\n",
      "('dances', 'and', 'to') [('have', 1)]\n",
      "('hard', 'to', 'have') [('her', 1)]\n",
      "('being', 'now', 'accepted') [('she', 1)]\n",
      "('bingley', 's', 'in') [('spite', 1)]\n",
      "('which', 'were', 'carried') [('to', 1)]\n"
     ]
    }
   ],
   "source": [
    "# Print top few most frequent ngrams\n",
    "def printNgrams(toprint, elementsToPrint):\n",
    "    for i in toprint:\n",
    "        print i\n",
    "        k=0\n",
    "        for j in toprint[i]:\n",
    "            k+=1\n",
    "            if i==1:\n",
    "                print j\n",
    "            else:\n",
    "                print j, toprint[i][j]\n",
    "            if k>elementsToPrint:\n",
    "                break\n",
    "printNgrams(ngrams,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating text using n-gram models with n>=3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool! Okay, let's see a selection of sentences for N-grams with N = 2 to 10 and a few starting words!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating 2-gram list... Done\n",
      "  2-gram: \" and the whole party were to be so much as to be so much as to \"\n",
      "  2-gram: \" he had been so much as to be so much as to be so much as \"\n",
      "  2-gram: \" she was not be so much as to be so much as to be so much \"\n",
      "  2-gram: \" when she was not be so much as to be so much as to be so \"\n",
      "  2-gram: \" john with the whole party were to be so much as to be so much as \"\n",
      "  2-gram: \" never be so much as to be so much as to be so much as to \"\n",
      "  2-gram: \" i am sure i am sure i am sure i am sure i am sure i \"\n",
      "  2-gram: \" how much as to be so much as to be so much as to be so \"\n",
      "\n",
      "Generating 3-gram list... Done\n",
      "  3-gram: \" and the other day we had better have taken you to be in london and last \"\n",
      "  3-gram: \" he had been so fortunate as to the project gutenberg tm electronic works in creating the \"\n",
      "  3-gram: \" she was not in the world and everybody said how pleasant it is not to be \"\n",
      "  3-gram: \" when she had been so fortunate as to the project gutenberg tm electronic works in creating \"\n",
      "  3-gram: \" john with the utmost civility and even a third it seemed the only one of the \"\n",
      "  3-gram: \" never be happy even supposing the best of the family were indebted for their daughter to \"\n",
      "  3-gram: \" i am sure i shall not be in london and last summer did mr darcy s \"\n",
      "  3-gram: \" how much i shall not be in london and last summer did mr darcy s letter \"\n",
      "\n",
      "Generating 4-gram list... Done\n",
      "  4-gram: \" and the other that i had not been so very agreeable he would have thought of \"\n",
      "  4-gram: \" he had been used to look in hertfordshire than as she had been blind partial prejudiced \"\n",
      "  4-gram: \" she was not very promising i am very glad to see them on the very day \"\n",
      "  4-gram: \" when she had finished it what a letter is this to be endured but it must \"\n",
      "  4-gram: \" john with the young ladies who introduced him to her friends this was a lucky recollection \"\n",
      "  4-gram: \" never be happy without him so much the better i hope they will not meet at \"\n",
      "  4-gram: \" i am sure i shall not be like other travellers without being able to retort on \"\n",
      "  4-gram: \" how much i shall have to conceal their journey was performed without much conversation or any \"\n",
      "\n",
      "Generating 5-gram list... Done\n",
      "  5-gram: \" and the other that i had in defiance of various claims in defiance of honour and \"\n",
      "  5-gram: \" he had been used to look in hertfordshire than as she had seen him last in \"\n",
      "  5-gram: \" she was not very sanguine in expecting it the application was a something to look forward \"\n",
      "  5-gram: \" when she had finished it what a letter is this to be written at such a \"\n",
      "  5-gram: \" john with the young ladies to the door of mr phillip s house and then made \"\n",
      "  5-gram: \" never be happy without him so think it no harm to be off you need not \"\n",
      "  5-gram: \" i am sure i shall not and i think it is very impertinent of him to \"\n",
      "  5-gram: \" how much i shall have to conceal their journey was performed without much conversation or any \"\n",
      "\n",
      "Generating 6-gram list... Done\n",
      "  6-gram: \" and the other that i had in defiance of various claims in defiance of honour and \"\n",
      "  6-gram: \" he had been used to look in hertfordshire than as she had seen him at pemberley \"\n",
      "  6-gram: \" she was not very sanguine in expecting it the application was a something to look forward \"\n",
      "  6-gram: \" when she had finished it what a letter is this to be written at such a \"\n",
      "  6-gram: \" john with the young ladies mrs collins i am glad it occurred to me to mention \"\n",
      "  6-gram: \" never be happy without him so think it no harm to be off you need not \"\n",
      "  6-gram: \" i am sure i shall not and i think it is very impertinent of him to \"\n",
      "  6-gram: \" how much i shall have to conceal their journey was performed without much conversation or any \"\n",
      "\n",
      "Generating 7-gram list... Done\n",
      "  7-gram: \" and the other that i had in defiance of various claims in defiance of honour and \"\n",
      "  7-gram: \" he had been used to look in hertfordshire than as she had seen him at pemberley \"\n",
      "  7-gram: \" she was not very sanguine in expecting it the application was a something to look forward \"\n",
      "  7-gram: \" when she had finished it what a letter is this to be written at such a \"\n",
      "  7-gram: \" john with the young ladies mrs collins i am glad it occurred to me to mention \"\n",
      "  7-gram: \" never be happy without him so think it no harm to be off you need not \"\n",
      "  7-gram: \" i am sure i shall not and i think it is very impertinent of him to \"\n",
      "  7-gram: \" how much i shall have to conceal their journey was performed without much conversation or any \"\n",
      "\n",
      "Generating 8-gram list... Done\n",
      "  8-gram: \" and the other that i had in defiance of various claims in defiance of honour and \"\n",
      "  8-gram: \" he had been used to look in hertfordshire than as she had seen him at pemberley \"\n",
      "  8-gram: \" she was not very sanguine in expecting it the application was a something to look forward \"\n",
      "  8-gram: \" when she had finished it what a letter is this to be written at such a \"\n",
      "  8-gram: \" john with the young ladies mrs collins i am glad it occurred to me to mention \"\n",
      "  8-gram: \" never be happy without him so think it no harm to be off you need not \"\n",
      "  8-gram: \" i am sure i shall not and i think it is very impertinent of him to \"\n",
      "  8-gram: \" how much i shall have to conceal their journey was performed without much conversation or any \"\n",
      "\n",
      "Generating 9-gram list... Done\n",
      "  9-gram: \" and the other that i had in defiance of various claims in defiance of honour and \"\n",
      "  9-gram: \" he had been used to look in hertfordshire than as she had seen him at pemberley \"\n",
      "  9-gram: \" she was not very sanguine in expecting it the application was a something to look forward \"\n",
      "  9-gram: \" when she had finished it what a letter is this to be written at such a \"\n",
      "  9-gram: \" john with the young ladies mrs collins i am glad it occurred to me to mention \"\n",
      "  9-gram: \" never be happy without him so think it no harm to be off you need not \"\n",
      "  9-gram: \" i am sure i shall not and i think it is very impertinent of him to \"\n",
      "  9-gram: \" how much i shall have to conceal their journey was performed without much conversation or any \"\n"
     ]
    }
   ],
   "source": [
    "def getUpToNGramSentenceMax(gram, word, n = 50):\n",
    "    #hist is a window of previous few words\n",
    "    hist=list()\n",
    "    maxHistorySize=len(gram)-1\n",
    "    for i in xrange(n):\n",
    "        if len(hist)>=maxHistorySize:\n",
    "            hist.pop(0)\n",
    "        hist.append(word)\n",
    "        found=0\n",
    "        for j in xrange(len(hist)): #start with longest possible history\n",
    "            key=tuple(hist[j:])\n",
    "            choices = [gram[len(key)+1][element] for element in gram[len(key)+1] if element == key]\n",
    "            if choices:\n",
    "                found=1\n",
    "                break\n",
    "        if found==0:\n",
    "            break\n",
    "        word=choices[0][0][0]\n",
    "        print word,\n",
    "\n",
    "for n in xrange(2,10):\n",
    "    print\n",
    "    print \"Generating %d-gram list...\" % n,\n",
    "    ngram=generateUpToNgram(n)\n",
    "    print \"Done\"\n",
    "    # Try out a bunch of sentences\n",
    "    for word in ['and', 'he', 'she', 'when', 'john', 'never', 'i', 'how']:\n",
    "        print \"  %d-gram: \\\"\" % n, word,\n",
    "        getUpToNGramSentenceMax(ngram, word, 15)\n",
    "        print \"\\\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating 2-gram list... Done\n",
      "  2-gram: \" and indifferent indeed not know what do not time but this agreement you to be at \"\n",
      "  2-gram: \" he went out the family they had mentioned his judgement controverted she could be highly esteemed \"\n",
      "  2-gram: \" she added may be generally bestowed my going away as my side of appearing on you \"\n",
      "  2-gram: \" when he must be her and prefer a countenance such as well is so odd a \"\n",
      "  2-gram: \" john with pleasure in nothing escaped his wife s marriage of about his sisters behind he \"\n",
      "  2-gram: \" never in all means capital one of seeing her excessive commendation of fashion so very proper \"\n",
      "  2-gram: \" i should like to give me i have been in compliment to let me no scruple \"\n",
      "  2-gram: \" how much beauty but there be doubted whether they could ever be supposed marriage he had \"\n",
      "\n",
      "Generating 3-gram list... Done\n",
      "  3-gram: \" and employees expend considerable effort much paperwork and many inquiries to make him marry her had \"\n",
      "  3-gram: \" he was totally ignorant of what she had known anything of the project gutenberg tm concept \"\n",
      "  3-gram: \" she performed her duty and to deserve to be encouraged as by wickham i can offer \"\n",
      "  3-gram: \" when i am very far from accusing you of books and the laurel hedge everything declared \"\n",
      "  3-gram: \" john with the invalid who continued though with his happiness they may wish many things besides \"\n",
      "  3-gram: \" never come back lizzy their party but mr collins meanwhile was giving way to such very \"\n",
      "  3-gram: \" i think there cannot be too much attention in the marriage of her uncle and aunt \"\n",
      "  3-gram: \" how far she wished him to talk to the girls to their mother was talking to \"\n",
      "\n",
      "Generating 4-gram list... Done\n",
      "  4-gram: \" and nonsense of others affectation of candour is common enough one meets with it everywhere but \"\n",
      "  4-gram: \" he had done and the boys were relieved from their apprehension of charlotte s dying an \"\n",
      "  4-gram: \" she was the happiest creature in the room from the sideboard to the fender to give \"\n",
      "  4-gram: \" when mrs bennet was restored to her usual querulous serenity and by the middle of the \"\n",
      "  4-gram: \" john with the young ladies mrs collins i am glad to find that she was much \"\n",
      "  4-gram: \" never resent her behaviour as any affront seated himself at another table with mr bennet and \"\n",
      "  4-gram: \" i had heard it was a circumstance which must prejudice her against him i am certainly \"\n",
      "  4-gram: \" how she liked it the housekeeper came forward and told them it was a very genteel \"\n",
      "\n",
      "Generating 5-gram list... Done\n",
      "  5-gram: \" and since i have had the pleasure of seeing you at pemberley to day oh yes \"\n",
      "  5-gram: \" he saw only the father the ladies were somewhat more fortunate for they had the advantage \"\n",
      "  5-gram: \" she went away i was perfectly resolved to continue the acquaintance no longer i pity though \"\n",
      "  5-gram: \" when the visit was paid and she had seen miss bingley i did not think you \"\n",
      "  5-gram: \" john told us mr darcy was here when you sent for us was it so yes \"\n",
      "  5-gram: \" never do more than like her if she does not object to them they can be \"\n",
      "  5-gram: \" i dare say but it would not be very likely to promote sisterly affection or delicacy \"\n",
      "  5-gram: \" how long his regiment is there for i suppose you have heard of it indeed you \"\n",
      "\n",
      "Generating 6-gram list... Done\n",
      "  6-gram: \" and public places the country is a vast deal pleasanter is it not mr bingley when \"\n",
      "  6-gram: \" he how near it may be to mine i cannot pretend to say you think it \"\n",
      "  6-gram: \" she been able to receive them into her house they would have taken up their abode \"\n",
      "  6-gram: \" when elizabeth had rejoiced over wickham s departure she found little other cause for satisfaction in \"\n",
      "  6-gram: \" john with the young ladies mrs collins i am glad it occurred to me to mention \"\n",
      "  6-gram: \" never see you again if you do elizabeth could not but smile at such a conclusion \"\n",
      "  6-gram: \" i have but one mind and one way of thinking there is in everything a most \"\n",
      "  6-gram: \" how can mr bingley who seems good humour itself and is i really believe truly amiable \"\n",
      "\n",
      "Generating 7-gram list... Done\n",
      "  7-gram: \" and commiseration let me call your maid is there nothing you could take to give you \"\n",
      "  7-gram: \" he coming home and without poor lydia she cried sure he will not leave london before \"\n",
      "  7-gram: \" she does not object to it why should we her not objecting does not justify him \"\n",
      "  7-gram: \" when she should enter the room and this is always the way with him she added \"\n",
      "  7-gram: \" john told us mr darcy was here when you sent for us was it so yes \"\n",
      "  7-gram: \" never alluded to but as no such delicacy restrained her mother an hour seldom passed in \"\n",
      "  7-gram: \" i am so happy in a short time i shall have a daughter married mrs wickham \"\n",
      "  7-gram: \" how he lived i know not but last summer he was again most painfully obtruded on \"\n",
      "\n",
      "Generating 8-gram list... Done\n",
      "  8-gram: \" and the three young ladies set off together if we make haste said lydia as they \"\n",
      "  8-gram: \" he received at the other table between elizabeth and lydia at first there seemed danger of \"\n",
      "  8-gram: \" she will not have mr collins and mr collins begins to say that he will not \"\n",
      "  8-gram: \" when in spots where the opening of the trees gave the eye power to wander were \"\n",
      "  8-gram: \" john with the young ladies mrs collins i am glad it occurred to me to mention \"\n",
      "  8-gram: \" never felt so strongly as now the disadvantages which must attend the children of so unsuitable \"\n",
      "  8-gram: \" i hope it is not true a great many changes have happened in the neighbourhood since \"\n",
      "  8-gram: \" how eagerly she went through them and what a contrariety of emotion they excited her feelings \"\n",
      "\n",
      "Generating 9-gram list... Done\n",
      "  9-gram: \" and she could hardly suppress a smile at his being now seeking the acquaintance of some \"\n",
      "  9-gram: \" he gave up everything to be of use to the late mr darcy and devoted all \"\n",
      "  9-gram: \" she might reasonably hope to hear again the promised letter of thanks from mr collins arrived \"\n",
      "  9-gram: \" when even the acquaintance of those to whom his attentions were addressed would draw down the \"\n",
      "  9-gram: \" john told us mr darcy was here when you sent for us was it so yes \"\n",
      "  9-gram: \" never have considered the distance as one of the advantages of the match cried elizabeth i \"\n",
      "  9-gram: \" i long to see her and to see dear wickham too but the clothes the wedding \"\n",
      "  9-gram: \" how could you begin said she i can comprehend your going on charmingly when you had \"\n"
     ]
    }
   ],
   "source": [
    "def getUpToNGramSentenceRandom(gram, word, n = 50):\n",
    "    hist=list()\n",
    "    maxHistorySize=len(gram)-1\n",
    "    for i in xrange(n):\n",
    "        if len(hist)>=maxHistorySize:\n",
    "            hist.pop(0)\n",
    "        hist.append(word)\n",
    "        found=0\n",
    "        for j in xrange(len(hist)):\n",
    "            key=tuple(hist[j:])\n",
    "            choices = [gram[len(key)+1][element] for element in gram[len(key)+1] if element == key]\n",
    "            if choices:\n",
    "                found=1\n",
    "                break\n",
    "        if found==0:\n",
    "            break\n",
    "        word = weighted_choice(choices[0])\n",
    "        print word,\n",
    "\n",
    "for n in xrange(2,10):\n",
    "    print\n",
    "    print \"Generating %d-gram list...\" % n,\n",
    "    ngram=generateUpToNgram(n)\n",
    "    print \"Done\"\n",
    "    # Try out a bunch of sentences\n",
    "    for word in ['and', 'he', 'she', 'when', 'john', 'never', 'i', 'how']:\n",
    "        print \"  %d-gram: \\\"\" % n, word,\n",
    "        getUpToNGramSentenceRandom(ngram, word, 15)\n",
    "        print \"\\\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can clearly see the sentences getting better and better with larger n-grams, this correlates to the ngram having more foresight into the sentence structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating 50-gram list... Done\n"
     ]
    }
   ],
   "source": [
    "# Generate 50gram list\n",
    "n = 50\n",
    "print\n",
    "print \"Generating %d-gram list...\" % n,\n",
    "gram50 = generateUpToNgram(n)\n",
    "print \"Done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  50-gram: \" she only wanted to know how far she wished that welfare to depend upon herself and how far it would be for the happiness of both that she should employ the power which her fancy told her she still possessed of bringing on her the renewal of his addresses it had been settled in the evening between the aunt and the niece that such a striking civility as miss darcy s in coming to see them on the very day of her arrival at pemberley for she had reached it only to a late breakfast ought to be imitated though \"\n",
      "  50-gram: \" never failed coming to inform them of though it happened almost every day she not unfrequently stopped at the parsonage and had a few minutes conversation with charlotte but was scarcely ever prevailed upon to get out very few days passed in which mr collins did not walk to rosings and not many in which his wife did not think it necessary to go likewise and till elizabeth recollected that there might be other family livings to be disposed of she could not understand the sacrifice of so many hours now and then they were honoured with a call from \"\n",
      "  50-gram: \" had seen him at pemberley but perhaps he could not in her mother s presence be what he was before her uncle and aunt it was a painful but not an improbable conjecture bingley she had likewise seen for an instant and in that short period saw him looking both pleased and embarrassed he was received by mrs bennet with a degree of civility which made her two daughters ashamed especially when contrasted with the cold and ceremonious politeness of her curtsey and address to his friend elizabeth particularly who knew that her mother owed to the latter the preservation \"\n",
      "  50-gram: \" she was discontented she fancied herself nervous the business of her life was to get her daughters married its solace was visiting and news chapter mr bennet was among the earliest of those who waited on mr bingley he had always intended to visit him though to the last always assuring his wife that he should not go and till the evening after the visit was paid she had no knowledge of it it was then disclosed in the following manner observing his second daughter employed in trimming a hat he suddenly addressed her with i hope mr bingley will \"\n",
      "  50-gram: \" with the young ladies mrs collins i am glad it occurred to me to mention it for it would really be discreditable to you to let them go alone my uncle is to send a servant for us oh your uncle he keeps a man servant does he i am very glad you have somebody who thinks of these things where shall you change horses oh bromley of course if you mention my name at the bell you will be attended to lady catherine had many other questions to ask respecting their journey and as she did not answer them \"\n",
      "  50-gram: \" even be mentioned by any of us these are heavy misfortunes replied elizabeth but the wife of mr darcy must have such extraordinary sources of happiness necessarily attached to her situation that she could upon the whole have no cause to repine obstinate headstrong girl i am ashamed of you is this your gratitude for my attentions to you last spring is nothing due to me on that score let us sit down you are to understand miss bennet that i came here with the determined resolution of carrying my purpose nor will i be dissuaded from it i have \"\n",
      "  50-gram: \" have your respected mother s permission for this address you can hardly doubt the purport of my discourse however your natural delicacy may lead you to dissemble my attentions have been too marked to be mistaken almost as soon as i entered the house i singled you out as the companion of my future life but before i am run away with by my feelings on this subject perhaps it would be advisable for me to state my reasons for marrying and moreover for coming into hertfordshire with the design of selecting a wife as i certainly did the idea \"\n",
      "  50-gram: \" they have tortured me though it was some time i confess before i was reasonable enough to allow their justice i was certainly very far from expecting them to make so strong an impression i had not the smallest idea of their being ever felt in such a way i can easily believe it you thought me then devoid of every proper feeling i am sure you did the turn of your countenance i shall never forget as you said that i could not have addressed you in any possible way that would induce you to accept me oh do \"\n"
     ]
    }
   ],
   "source": [
    "# Try out a bunch of sentences\n",
    "for word in ['and', 'he', 'she', 'when', 'john', 'never', 'i', 'how']:\n",
    "    print \"  %d-gram: \\\"\" % n,\n",
    "    getUpToNGramSentenceRandom(gram50, word, 100)\n",
    "    print \"\\\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Laplace Smoothed n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating 2-gram list...\n",
      "1\n",
      "(('the',), 4507)\n",
      "(('to',), 4243)\n",
      "(('of',), 3730)\n",
      "(('and',), 3658)\n",
      "(('her',), 2225)\n",
      "(('i',), 2070)\n",
      "(('a',), 2012)\n",
      "(('in',), 1937)\n",
      "(('was',), 1847)\n",
      "(('she',), 1710)\n",
      "(('that',), 1594)\n",
      "2\n",
      "('resolve',) [('very', 1), ('to', 1)]\n",
      "('waited',) [('on', 3), ('for', 2), ('only', 1), ('at', 1)]\n",
      "('housekeeping',) [('i', 1), ('when', 1), ('her', 1)]\n",
      "('defective',) [('the', 1), ('work', 1), ('or', 1), ('you', 1)]\n",
      "('intricate',) [('character', 1), ('characters', 1)]\n",
      "('wavering',) [('before', 1)]\n",
      "('now',) [('i', 9), ('and', 8), ('to', 6), ('be', 6), ('have', 5), ('as', 4), ('when', 4), ('a', 4), ('the', 4), ('she', 3), ('come', 3), ('made', 3), ('in', 3), ('indeed', 2), ('only', 2), ('do', 2), ('because', 2), ('absolutely', 2), ('said', 2), ('for', 2), ('told', 2), ('we', 2), ('put', 2), ('on', 2), ('of', 2), ('fast', 2), ('been', 2), ('passed', 2), ('was', 2), ('but', 2), ('convinced', 2), ('determined', 2), ('it', 2), ('at', 2), ('saw', 2), ('began', 2), ('so', 2), ('despise', 1), ('being', 1), ('openly', 1), ('quartered', 1), ('now', 1), ('thank', 1), ('except', 1), ('georgiana', 1), ('going', 1), ('struck', 1), ('wished', 1), ('seeking', 1), ('very', 1), ('kitty', 1), ('mention', 1), ('suffering', 1), ('nearly', 1), ('they', 1), ('fall', 1), ('accepted', 1), ('recollected', 1), ('necessary', 1), ('heightened', 1), ('lost', 1), ('doubled', 1), ('anxious', 1), ('become', 1), ('approaching', 1), ('some', 1), ('feel', 1), ('are', 1), ('escape', 1), ('employment', 1), ('provided', 1), ('what', 1), ('learnt', 1), ('appear', 1), ('seriously', 1), ('free', 1), ('let', 1), ('here', 1), ('teach', 1), ('painfully', 1), ('obtained', 1), ('about', 1), ('asked', 1), ('first', 1), ('appeared', 1), ('brought', 1), ('rendering', 1), ('done', 1), ('lizzy', 1), ('given', 1), ('rushed', 1), ('there', 1), ('add', 1), ('much', 1), ('expected', 1), ('entered', 1), ('tell', 1), ('more', 1), ('enough', 1), ('relieved', 1), ('occurred', 1), ('applied', 1), ('say', 1), ('seated', 1), ('near', 1), ('suppose', 1), ('mr', 1), ('my', 1), ('called', 1), ('gone', 1), ('give', 1), ('almost', 1), ('is', 1), ('an', 1), ('high', 1), ('if', 1), ('forced', 1), ('productive', 1), ('perhaps', 1), ('that', 1), ('concealed', 1), ('take', 1), ('you', 1), ('several', 1), ('added', 1), ('elizabeth', 1), ('living', 1), ('most', 1), ('smiled', 1), ('nothing', 1), ('walked', 1), ('complaining', 1), ('dissolved', 1), ('having', 1), ('came', 1), ('know', 1)]\n",
      "('thwarted',) [('so', 1)]\n",
      "('weighed',) [('on', 1), ('every', 1)]\n",
      "('rate',) [('a', 1), ('you', 1), ('there', 1), ('was', 1), ('she', 1)]\n",
      "('struggle',) [('in', 1)]\n",
      "1\n",
      "(('the',), 0.034040368191737586)\n",
      "(('to',), 0.03204687724173343)\n",
      "(('of',), 0.02817316187297536)\n",
      "(('and',), 0.027629482522974227)\n",
      "(('her',), 0.01680875323753502)\n",
      "(('i',), 0.015638332414615912)\n",
      "(('a',), 0.015200368493781667)\n",
      "(('in',), 0.014634035837530488)\n",
      "(('was',), 0.013954436650029071)\n",
      "(('she',), 0.01291993566461025)\n",
      "(('that',), 0.012044007822941759)\n",
      "2\n",
      "('resolve',) [('very', 0.0003061849357011635), ('to', 0.0003061849357011635)]\n",
      "('waited',) [('on', 0.0006119014838610984), ('for', 0.0004589261128958238), ('only', 0.0003059507419305492), ('at', 0.0003059507419305492)]\n",
      "('mingled',) [('incredulity', 0.0003062318174858368)]\n",
      "('defective',) [('the', 0.00030609121518212427), ('work', 0.00030609121518212427), ('or', 0.00030609121518212427), ('you', 0.00030609121518212427)]\n",
      "('intricate',) [('character', 0.0003061849357011635), ('characters', 0.0003061849357011635)]\n",
      "('wavering',) [('before', 0.0003062318174858368)]\n",
      "('now',) [('i', 0.0014838996883810654), ('and', 0.001335509719542959), ('to', 0.0010387297818667458), ('be', 0.0010387297818667458), ('have', 0.0008903398130286393), ('as', 0.0007419498441905327), ('when', 0.0007419498441905327), ('a', 0.0007419498441905327), ('the', 0.0007419498441905327), ('she', 0.0005935598753524262), ('come', 0.0005935598753524262), ('made', 0.0005935598753524262), ('in', 0.0005935598753524262), ('indeed', 0.00044516990651431964), ('only', 0.00044516990651431964), ('do', 0.00044516990651431964), ('because', 0.00044516990651431964), ('absolutely', 0.00044516990651431964), ('said', 0.00044516990651431964), ('for', 0.00044516990651431964), ('told', 0.00044516990651431964), ('we', 0.00044516990651431964), ('put', 0.00044516990651431964), ('on', 0.00044516990651431964), ('of', 0.00044516990651431964), ('fast', 0.00044516990651431964), ('been', 0.00044516990651431964), ('passed', 0.00044516990651431964), ('was', 0.00044516990651431964), ('but', 0.00044516990651431964), ('convinced', 0.00044516990651431964), ('determined', 0.00044516990651431964), ('it', 0.00044516990651431964), ('at', 0.00044516990651431964), ('saw', 0.00044516990651431964), ('began', 0.00044516990651431964), ('so', 0.00044516990651431964), ('despise', 0.0002967799376762131), ('being', 0.0002967799376762131), ('openly', 0.0002967799376762131), ('quartered', 0.0002967799376762131), ('now', 0.0002967799376762131), ('thank', 0.0002967799376762131), ('except', 0.0002967799376762131), ('georgiana', 0.0002967799376762131), ('going', 0.0002967799376762131), ('struck', 0.0002967799376762131), ('wished', 0.0002967799376762131), ('seeking', 0.0002967799376762131), ('very', 0.0002967799376762131), ('kitty', 0.0002967799376762131), ('mention', 0.0002967799376762131), ('suffering', 0.0002967799376762131), ('nearly', 0.0002967799376762131), ('they', 0.0002967799376762131), ('fall', 0.0002967799376762131), ('accepted', 0.0002967799376762131), ('recollected', 0.0002967799376762131), ('necessary', 0.0002967799376762131), ('heightened', 0.0002967799376762131), ('lost', 0.0002967799376762131), ('doubled', 0.0002967799376762131), ('anxious', 0.0002967799376762131), ('become', 0.0002967799376762131), ('approaching', 0.0002967799376762131), ('some', 0.0002967799376762131), ('are', 0.0002967799376762131), ('escape', 0.0002967799376762131), ('employment', 0.0002967799376762131), ('provided', 0.0002967799376762131), ('what', 0.0002967799376762131), ('learnt', 0.0002967799376762131), ('appear', 0.0002967799376762131), ('seriously', 0.0002967799376762131), ('here', 0.0002967799376762131), ('let', 0.0002967799376762131), ('free', 0.0002967799376762131), ('teach', 0.0002967799376762131), ('painfully', 0.0002967799376762131), ('obtained', 0.0002967799376762131), ('about', 0.0002967799376762131), ('asked', 0.0002967799376762131), ('first', 0.0002967799376762131), ('feel', 0.0002967799376762131), ('brought', 0.0002967799376762131), ('rendering', 0.0002967799376762131), ('done', 0.0002967799376762131), ('lizzy', 0.0002967799376762131), ('appeared', 0.0002967799376762131), ('given', 0.0002967799376762131), ('rushed', 0.0002967799376762131), ('there', 0.0002967799376762131), ('add', 0.0002967799376762131), ('much', 0.0002967799376762131), ('expected', 0.0002967799376762131), ('entered', 0.0002967799376762131), ('tell', 0.0002967799376762131), ('more', 0.0002967799376762131), ('enough', 0.0002967799376762131), ('relieved', 0.0002967799376762131), ('occurred', 0.0002967799376762131), ('applied', 0.0002967799376762131), ('say', 0.0002967799376762131), ('seated', 0.0002967799376762131), ('near', 0.0002967799376762131), ('suppose', 0.0002967799376762131), ('mr', 0.0002967799376762131), ('my', 0.0002967799376762131), ('called', 0.0002967799376762131), ('gone', 0.0002967799376762131), ('give', 0.0002967799376762131), ('almost', 0.0002967799376762131), ('is', 0.0002967799376762131), ('an', 0.0002967799376762131), ('high', 0.0002967799376762131), ('if', 0.0002967799376762131), ('forced', 0.0002967799376762131), ('productive', 0.0002967799376762131), ('perhaps', 0.0002967799376762131), ('that', 0.0002967799376762131), ('concealed', 0.0002967799376762131), ('take', 0.0002967799376762131), ('you', 0.0002967799376762131), ('several', 0.0002967799376762131), ('added', 0.0002967799376762131), ('elizabeth', 0.0002967799376762131), ('living', 0.0002967799376762131), ('most', 0.0002967799376762131), ('smiled', 0.0002967799376762131), ('nothing', 0.0002967799376762131), ('walked', 0.0002967799376762131), ('complaining', 0.0002967799376762131), ('dissolved', 0.0002967799376762131), ('having', 0.0002967799376762131), ('came', 0.0002967799376762131), ('know', 0.0002967799376762131)]\n",
      "('thwarted',) [('so', 0.0003062318174858368)]\n",
      "('weighed',) [('on', 0.0003061849357011635), ('every', 0.0003061849357011635)]\n",
      "('rate',) [('a', 0.000306044376434583), ('you', 0.000306044376434583), ('there', 0.000306044376434583), ('was', 0.000306044376434583), ('she', 0.000306044376434583)]\n",
      "('struggle',) [('in', 0.0003062318174858368)]\n"
     ]
    }
   ],
   "source": [
    "def getLaplaceSmoothedUpToNGramModel(n=2, printModels=True):\n",
    "    # Generate ngram list\n",
    "    print\n",
    "    print \"Generating %d-gram list...\\n\" % n,\n",
    "    ngram = generateUpToNgram(n)\n",
    "    if printModels:\n",
    "        printNgrams(ngram,10)\n",
    "    smoothed=dict()\n",
    "    total=0\n",
    "    for i in xrange(1,n+1):\n",
    "        grams=ngram[i]\n",
    "        smoothedGrams=dict()\n",
    "        if i==1:\n",
    "            total=0\n",
    "            for key, val in grams:\n",
    "                total+=val\n",
    "            #print(total)\n",
    "            for key, val in grams:\n",
    "                smoothedGrams[key]=(float(val)+1)/(float(total)+len(grams))\n",
    "            smoothedGrams = sorted(smoothedGrams.items(), key=lambda (_, count): -count)\n",
    "        else:\n",
    "            for key in grams:\n",
    "                smoothedLocalDict=dict()\n",
    "                localDict=grams[key]\n",
    "                total=0\n",
    "                for k,v in localDict:\n",
    "                    total+=v\n",
    "                for k,v in localDict:\n",
    "                    smoothedLocalDict[k]=(float(v)+1)/(float(total)+len(ngram[1])) # ngram[1] because it defines the vocabulary\n",
    "                smoothedLocalDict = sorted(smoothedLocalDict.items(), key=lambda (_, count): -count)\n",
    "                smoothedGrams[key]=smoothedLocalDict\n",
    "        smoothed[i]=smoothedGrams\n",
    "    if printModels:\n",
    "        printNgrams(smoothed, 10)\n",
    "    return smoothed\n",
    "\n",
    "_=getLaplaceSmoothedUpToNGramModel(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Computing perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating 4-gram list...\n"
     ]
    }
   ],
   "source": [
    "ngram = getLaplaceSmoothedUpToNGramModel(4, False)\n",
    "#printNgrams(ngram,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13.49334689669374"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "13.49334689669374"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "and i\n",
      "['and'] i\n",
      "i will\n",
      "['i'] will\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9.399726570492227"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "and will\n",
      "['and'] will\n",
      "will i\n",
      "['will'] i\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "14.338080884761208"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "def computePerplexity(model, sentence, order):\n",
    "    toks=sentence.split(' ')\n",
    "    logprob=0.0\n",
    "    if order==1:\n",
    "        for t in toks:\n",
    "            val=dict(model[1])\n",
    "            logprob+=math.log(val[tuple([t])])\n",
    "        #print logprob\n",
    "    #modelSize=len(next(iter(model)))+1\n",
    "    else:\n",
    "        for i in xrange(0, len(toks)-order+1):\n",
    "            l=list()\n",
    "            for j in xrange(i, i+order):\n",
    "                print toks[j],\n",
    "            print\n",
    "            for j in xrange(i, i+order-1):\n",
    "                l.append(toks[j])\n",
    "            lastword=toks[i+order-1]\n",
    "            print l, lastword\n",
    "            val=dict(model[order])\n",
    "            #print(dict(val[tuple(l)])[lastword])\n",
    "            logprob+=math.log(dict(val[tuple(l)])[lastword])\n",
    "            logprob/=order\n",
    "            \n",
    "    return -logprob\n",
    "    \n",
    "computePerplexity(ngram, \"and i will\", 1)\n",
    "computePerplexity(ngram, \"and will i\", 1)\n",
    "computePerplexity(ngram, \"and i will\", 2)\n",
    "computePerplexity(ngram, \"and will i\", 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
