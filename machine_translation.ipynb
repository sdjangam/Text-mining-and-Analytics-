{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Machine Translation**\n",
    "Machine Translation (MT) is the task of automatically converting one natural language into another, preserving the meaning of the input text, and producing fluent text in the output language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Required Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model \n",
    "from tensorflow.keras.layers import Input, LSTM, Dense\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **intializing some parameteres**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64 # Batch size for training\n",
    "epochs = 100    # Number of epochs to train for \n",
    "latent_dim = 256 # Latent dimensionality of the encoding space \n",
    "num_samples = 10000 # Number of Samples to Train\n",
    "\n",
    "# Path of the data txt file\n",
    "data_path = r'C:\\Users\\jgaur\\Tensorflow_Tut\\Machine_Translation\\fra-eng\\fra.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''Vectorize the data'''\n",
    "input_texts = []\n",
    "target_texts = []\n",
    "input_characters = set()\n",
    "target_characters = set()\n",
    "\n",
    "''' reading file '''\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    lines = f.read().split('\\n')\n",
    "\n",
    "''' preparing the data for training '''\n",
    "for line in lines[: min(num_samples, len(lines) -1)]:\n",
    "    input_text, target_text, _ = line.split('\\t')\n",
    "    target_text = '\\t' + target_text + '\\n'\n",
    "    input_texts.append(input_text)\n",
    "    target_texts.append(target_text)\n",
    "    for char in input_text:\n",
    "        if char not in input_characters:\n",
    "            input_characters.add(char)\n",
    "    for char in target_text:\n",
    "        if char not in target_characters:\n",
    "            target_characters.add(char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71\n",
      "94\n"
     ]
    }
   ],
   "source": [
    "print(len(input_characters))\n",
    "print(len(target_characters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_characters = sorted(list(input_characters))\n",
    "target_characters = sorted(list(target_characters))\n",
    "num_encoder_tokens = len(input_characters)\n",
    "num_decoder_tokens = len(target_characters))\n",
    "max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
    "max_encoder_seq_length = max([len(txt) for txt in input_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 10000\n",
      "Number of unique input tokens: 71\n",
      "Number of unique output tokerns: 94\n",
      "Max sequence length for inputs: 15\n",
      "Max sequence length for outputs: 59\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of samples:\", len(input_texts))\n",
    "print(\"Number of unique input tokens:\", num_encoder_tokens)\n",
    "print(\"Number of unique output tokerns:\", num_decoder_tokens)\n",
    "print(\"Max sequence length for inputs:\", max_encoder_seq_length)\n",
    "print(\"Max sequence length for outputs:\", max_decoder_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_token_index = dict(\n",
    "    [(char, i) for i, char in enumerate(input_characters)]\n",
    ")\n",
    "\n",
    "output_token_index = dict(\n",
    "    [(char, i) for i, char in enumerate(target_characters)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_token_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_data = np.zeros(\n",
    "    # total no. of sentence, max length of a sentence, total no. of english characters\n",
    "    (len(input_texts), max_encoder_seq_length, num_encoder_tokens), \n",
    "    dtype='float32'\n",
    ")\n",
    "\n",
    "decoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens), \n",
    "    dtype='float32'\n",
    ")\n",
    "\n",
    "decoder_output_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 15, 71)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_input_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_token_index[' ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (input_text, output_text) in enumerate(zip(input_texts, target_texts)):\n",
    "    for i_, char in enumerate(input_text):\n",
    "        encoder_input_data[i, i_, input_token_index[char]] = 1 \n",
    "    encoder_input_data[i, i_ + 1:, input_token_index[' ']] = 1\n",
    "    for i_, char in enumerate(output_text):\n",
    "        decoder_input_data[i, i_, output_token_index[char]] = 1\n",
    "        if i_ > 0:\n",
    "            # decoder_target data will be ahead by one timestep\n",
    "            # and will not include the start character\n",
    "            decoder_output_data[i, i_ - 1, output_token_index[char]] = 1\n",
    "    decoder_input_data[i, i_ + 1:, output_token_index[' ']] = 1\n",
    "    decoder_output_data[i, i_:, output_token_index[' ']] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_encoder_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Encoder**\n",
    "Encoder decoder models allow for a process in which a machine learning model generates a sentence describing an image. It receives the image as the input and outputs a sequence of words. This also works with videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"input_1:0\", shape=(None, None, 71), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Define an input sequence and process it\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "print(encoder_inputs)\n",
    "encoder = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "# We discard 'encoder_outputs and only keep the states\n",
    "encoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Decoder**\n",
    "`Decoder` means to convert a coded message into intelligible language. ... In the machine learning model, the role of the decoder will be to convert the two-dimensional vector into the output sequence, the English sentence. It is also built with RNN layers and a dense layer to predict the English word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the decoder, using 'encoder_states' as initial_state\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "# We set up our decoder to return  full output sequences, \n",
    "# and to return  internal states as well. We don't use the\n",
    "# return states in the training model, but we will use them in inference\n",
    "\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, \n",
    "                                    initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, None, 71)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            [(None, None, 94)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, 256), (None, 335872      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, None, 256),  359424      input_3[0][0]                    \n",
      "                                                                 lstm[0][1]                       \n",
      "                                                                 lstm[0][2]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, None, 94)     24158       lstm_1[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 719,454\n",
      "Trainable params: 719,454\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "125/125 [==============================] - 76s 609ms/step - loss: 1.1647 - accuracy: 0.7323 - val_loss: 1.0238 - val_accuracy: 0.7077\n",
      "Epoch 2/100\n",
      "125/125 [==============================] - 77s 613ms/step - loss: 0.8385 - accuracy: 0.7734 - val_loss: 0.8122 - val_accuracy: 0.7732\n",
      "Epoch 3/100\n",
      "125/125 [==============================] - 60s 484ms/step - loss: 0.6754 - accuracy: 0.8090 - val_loss: 0.7167 - val_accuracy: 0.7933\n",
      "Epoch 4/100\n",
      "125/125 [==============================] - 58s 462ms/step - loss: 0.5925 - accuracy: 0.8276 - val_loss: 0.6582 - val_accuracy: 0.8075\n",
      "Epoch 5/100\n",
      "125/125 [==============================] - 62s 493ms/step - loss: 0.5417 - accuracy: 0.8412 - val_loss: 0.6152 - val_accuracy: 0.8178\n",
      "Epoch 6/100\n",
      "125/125 [==============================] - 61s 491ms/step - loss: 0.5030 - accuracy: 0.8522 - val_loss: 0.5823 - val_accuracy: 0.8243\n",
      "Epoch 7/100\n",
      "125/125 [==============================] - 56s 451ms/step - loss: 0.4731 - accuracy: 0.8602 - val_loss: 0.5679 - val_accuracy: 0.8308\n",
      "Epoch 8/100\n",
      "125/125 [==============================] - 54s 435ms/step - loss: 0.4488 - accuracy: 0.8667 - val_loss: 0.5387 - val_accuracy: 0.8413\n",
      "Epoch 9/100\n",
      "125/125 [==============================] - 70s 559ms/step - loss: 0.4274 - accuracy: 0.8725 - val_loss: 0.5225 - val_accuracy: 0.8463\n",
      "Epoch 10/100\n",
      "125/125 [==============================] - 58s 461ms/step - loss: 0.4085 - accuracy: 0.8780 - val_loss: 0.5144 - val_accuracy: 0.8473\n",
      "Epoch 11/100\n",
      "125/125 [==============================] - 65s 522ms/step - loss: 0.3910 - accuracy: 0.8829 - val_loss: 0.4985 - val_accuracy: 0.8523\n",
      "Epoch 12/100\n",
      "125/125 [==============================] - 59s 471ms/step - loss: 0.3751 - accuracy: 0.8874 - val_loss: 0.4939 - val_accuracy: 0.8544\n",
      "Epoch 13/100\n",
      "125/125 [==============================] - 74s 595ms/step - loss: 0.3602 - accuracy: 0.8921 - val_loss: 0.4822 - val_accuracy: 0.8572\n",
      "Epoch 14/100\n",
      "125/125 [==============================] - 61s 486ms/step - loss: 0.3462 - accuracy: 0.8958 - val_loss: 0.4730 - val_accuracy: 0.8600\n",
      "Epoch 15/100\n",
      "125/125 [==============================] - 61s 488ms/step - loss: 0.3329 - accuracy: 0.9002 - val_loss: 0.4733 - val_accuracy: 0.8603\n",
      "Epoch 16/100\n",
      "125/125 [==============================] - 66s 526ms/step - loss: 0.3209 - accuracy: 0.9034 - val_loss: 0.4612 - val_accuracy: 0.8649\n",
      "Epoch 17/100\n",
      "125/125 [==============================] - 78s 623ms/step - loss: 0.3093 - accuracy: 0.9070 - val_loss: 0.4582 - val_accuracy: 0.8665\n",
      "Epoch 18/100\n",
      "125/125 [==============================] - 84s 668ms/step - loss: 0.2982 - accuracy: 0.9102 - val_loss: 0.4569 - val_accuracy: 0.8674\n",
      "Epoch 19/100\n",
      "125/125 [==============================] - 73s 584ms/step - loss: 0.2876 - accuracy: 0.9131 - val_loss: 0.4531 - val_accuracy: 0.8679\n",
      "Epoch 20/100\n",
      "125/125 [==============================] - 57s 455ms/step - loss: 0.2773 - accuracy: 0.9163 - val_loss: 0.4575 - val_accuracy: 0.8676\n",
      "Epoch 21/100\n",
      "125/125 [==============================] - 60s 479ms/step - loss: 0.2673 - accuracy: 0.9192 - val_loss: 0.4548 - val_accuracy: 0.8694\n",
      "Epoch 22/100\n",
      "125/125 [==============================] - 53s 421ms/step - loss: 0.2585 - accuracy: 0.9216 - val_loss: 0.4524 - val_accuracy: 0.8700\n",
      "Epoch 23/100\n",
      "125/125 [==============================] - 58s 462ms/step - loss: 0.2497 - accuracy: 0.9243 - val_loss: 0.4548 - val_accuracy: 0.8709\n",
      "Epoch 24/100\n",
      "125/125 [==============================] - 58s 463ms/step - loss: 0.2413 - accuracy: 0.9269 - val_loss: 0.4540 - val_accuracy: 0.8701\n",
      "Epoch 25/100\n",
      "125/125 [==============================] - 51s 407ms/step - loss: 0.2329 - accuracy: 0.9292 - val_loss: 0.4553 - val_accuracy: 0.8717\n",
      "Epoch 26/100\n",
      "125/125 [==============================] - 58s 463ms/step - loss: 0.2254 - accuracy: 0.9317 - val_loss: 0.4574 - val_accuracy: 0.8710\n",
      "Epoch 27/100\n",
      "125/125 [==============================] - 52s 415ms/step - loss: 0.2180 - accuracy: 0.9337 - val_loss: 0.4580 - val_accuracy: 0.8715\n",
      "Epoch 28/100\n",
      "125/125 [==============================] - 55s 443ms/step - loss: 0.2105 - accuracy: 0.9361 - val_loss: 0.4625 - val_accuracy: 0.8718\n",
      "Epoch 29/100\n",
      "125/125 [==============================] - 53s 427ms/step - loss: 0.2039 - accuracy: 0.9382 - val_loss: 0.4632 - val_accuracy: 0.8717\n",
      "Epoch 30/100\n",
      "125/125 [==============================] - 53s 426ms/step - loss: 0.1975 - accuracy: 0.9400 - val_loss: 0.4635 - val_accuracy: 0.8730\n",
      "Epoch 31/100\n",
      "125/125 [==============================] - 52s 420ms/step - loss: 0.1915 - accuracy: 0.9417 - val_loss: 0.4732 - val_accuracy: 0.8721\n",
      "Epoch 32/100\n",
      "125/125 [==============================] - 55s 442ms/step - loss: 0.1851 - accuracy: 0.9436 - val_loss: 0.4758 - val_accuracy: 0.8727\n",
      "Epoch 33/100\n",
      "125/125 [==============================] - 56s 448ms/step - loss: 0.1796 - accuracy: 0.9451 - val_loss: 0.4779 - val_accuracy: 0.8722\n",
      "Epoch 34/100\n",
      "125/125 [==============================] - 55s 436ms/step - loss: 0.1742 - accuracy: 0.9469 - val_loss: 0.4842 - val_accuracy: 0.8720\n",
      "Epoch 35/100\n",
      "125/125 [==============================] - 53s 427ms/step - loss: 0.1691 - accuracy: 0.9484 - val_loss: 0.4824 - val_accuracy: 0.8732\n",
      "Epoch 36/100\n",
      "125/125 [==============================] - 52s 414ms/step - loss: 0.1640 - accuracy: 0.9499 - val_loss: 0.4913 - val_accuracy: 0.8726\n",
      "Epoch 37/100\n",
      "125/125 [==============================] - 52s 415ms/step - loss: 0.1593 - accuracy: 0.9513 - val_loss: 0.4968 - val_accuracy: 0.8722\n",
      "Epoch 38/100\n",
      "125/125 [==============================] - 52s 416ms/step - loss: 0.1543 - accuracy: 0.9526 - val_loss: 0.4994 - val_accuracy: 0.8721\n",
      "Epoch 39/100\n",
      "125/125 [==============================] - 53s 421ms/step - loss: 0.1505 - accuracy: 0.9537 - val_loss: 0.5014 - val_accuracy: 0.8727\n",
      "Epoch 40/100\n",
      "125/125 [==============================] - 57s 457ms/step - loss: 0.1463 - accuracy: 0.9551 - val_loss: 0.5086 - val_accuracy: 0.8718\n",
      "Epoch 41/100\n",
      "125/125 [==============================] - 57s 455ms/step - loss: 0.1422 - accuracy: 0.9561 - val_loss: 0.5134 - val_accuracy: 0.8725\n",
      "Epoch 42/100\n",
      "125/125 [==============================] - 51s 406ms/step - loss: 0.1382 - accuracy: 0.9574 - val_loss: 0.5136 - val_accuracy: 0.8728\n",
      "Epoch 43/100\n",
      "125/125 [==============================] - 51s 410ms/step - loss: 0.1343 - accuracy: 0.9584 - val_loss: 0.5232 - val_accuracy: 0.8720\n",
      "Epoch 44/100\n",
      "125/125 [==============================] - 52s 419ms/step - loss: 0.1305 - accuracy: 0.9596 - val_loss: 0.5261 - val_accuracy: 0.8724\n",
      "Epoch 45/100\n",
      "125/125 [==============================] - 51s 408ms/step - loss: 0.1271 - accuracy: 0.9604 - val_loss: 0.5310 - val_accuracy: 0.8727\n",
      "Epoch 46/100\n",
      "125/125 [==============================] - 51s 409ms/step - loss: 0.1239 - accuracy: 0.9615 - val_loss: 0.5403 - val_accuracy: 0.8717\n",
      "Epoch 47/100\n",
      "125/125 [==============================] - 51s 411ms/step - loss: 0.1206 - accuracy: 0.9625 - val_loss: 0.5395 - val_accuracy: 0.8721\n",
      "Epoch 48/100\n",
      "125/125 [==============================] - 93s 743ms/step - loss: 0.1178 - accuracy: 0.9633 - val_loss: 0.5472 - val_accuracy: 0.8722\n",
      "Epoch 49/100\n",
      "125/125 [==============================] - 56s 446ms/step - loss: 0.1147 - accuracy: 0.9645 - val_loss: 0.5542 - val_accuracy: 0.8714\n",
      "Epoch 50/100\n",
      "125/125 [==============================] - 60s 480ms/step - loss: 0.1119 - accuracy: 0.9650 - val_loss: 0.5571 - val_accuracy: 0.8707\n",
      "Epoch 51/100\n",
      "125/125 [==============================] - 55s 442ms/step - loss: 0.1090 - accuracy: 0.9661 - val_loss: 0.5664 - val_accuracy: 0.8709\n",
      "Epoch 52/100\n",
      "125/125 [==============================] - 50s 402ms/step - loss: 0.1064 - accuracy: 0.9669 - val_loss: 0.5708 - val_accuracy: 0.8713\n",
      "Epoch 53/100\n",
      "125/125 [==============================] - 55s 439ms/step - loss: 0.1038 - accuracy: 0.9675 - val_loss: 0.5711 - val_accuracy: 0.8710\n",
      "Epoch 54/100\n",
      "125/125 [==============================] - 58s 462ms/step - loss: 0.1013 - accuracy: 0.9681 - val_loss: 0.5735 - val_accuracy: 0.8724\n",
      "Epoch 55/100\n",
      "125/125 [==============================] - 67s 533ms/step - loss: 0.0988 - accuracy: 0.9690 - val_loss: 0.5847 - val_accuracy: 0.8721\n",
      "Epoch 56/100\n",
      "125/125 [==============================] - 81s 645ms/step - loss: 0.0964 - accuracy: 0.9697 - val_loss: 0.5856 - val_accuracy: 0.8718\n",
      "Epoch 57/100\n",
      "125/125 [==============================] - 66s 532ms/step - loss: 0.0946 - accuracy: 0.9701 - val_loss: 0.5929 - val_accuracy: 0.8711\n",
      "Epoch 58/100\n",
      "125/125 [==============================] - 84s 674ms/step - loss: 0.0920 - accuracy: 0.9709 - val_loss: 0.5993 - val_accuracy: 0.8701\n",
      "Epoch 59/100\n",
      "125/125 [==============================] - 67s 540ms/step - loss: 0.0903 - accuracy: 0.9714 - val_loss: 0.5990 - val_accuracy: 0.8716\n",
      "Epoch 60/100\n",
      "125/125 [==============================] - 68s 543ms/step - loss: 0.0883 - accuracy: 0.9718 - val_loss: 0.6036 - val_accuracy: 0.8717\n",
      "Epoch 61/100\n",
      "125/125 [==============================] - 67s 533ms/step - loss: 0.0863 - accuracy: 0.9725 - val_loss: 0.6135 - val_accuracy: 0.8697\n",
      "Epoch 62/100\n",
      "125/125 [==============================] - 72s 580ms/step - loss: 0.0844 - accuracy: 0.9729 - val_loss: 0.6108 - val_accuracy: 0.8716\n",
      "Epoch 63/100\n",
      "125/125 [==============================] - 66s 526ms/step - loss: 0.0831 - accuracy: 0.9734 - val_loss: 0.6187 - val_accuracy: 0.8711\n",
      "Epoch 64/100\n",
      "125/125 [==============================] - 54s 435ms/step - loss: 0.0807 - accuracy: 0.9740 - val_loss: 0.6237 - val_accuracy: 0.8708\n",
      "Epoch 65/100\n",
      "125/125 [==============================] - 56s 450ms/step - loss: 0.0789 - accuracy: 0.9747 - val_loss: 0.6253 - val_accuracy: 0.8709\n",
      "Epoch 66/100\n",
      "125/125 [==============================] - 55s 438ms/step - loss: 0.0773 - accuracy: 0.9751 - val_loss: 0.6318 - val_accuracy: 0.8709\n",
      "Epoch 67/100\n",
      "125/125 [==============================] - 54s 433ms/step - loss: 0.0756 - accuracy: 0.9756 - val_loss: 0.6357 - val_accuracy: 0.8701\n",
      "Epoch 68/100\n",
      "125/125 [==============================] - 58s 462ms/step - loss: 0.0744 - accuracy: 0.9759 - val_loss: 0.6413 - val_accuracy: 0.8703\n",
      "Epoch 69/100\n",
      "125/125 [==============================] - 82s 655ms/step - loss: 0.0727 - accuracy: 0.9764 - val_loss: 0.6402 - val_accuracy: 0.8708\n",
      "Epoch 70/100\n",
      "125/125 [==============================] - 86s 686ms/step - loss: 0.0713 - accuracy: 0.9769 - val_loss: 0.6513 - val_accuracy: 0.8702\n",
      "Epoch 71/100\n",
      "125/125 [==============================] - 64s 509ms/step - loss: 0.0700 - accuracy: 0.9770 - val_loss: 0.6533 - val_accuracy: 0.8709\n",
      "Epoch 72/100\n",
      "125/125 [==============================] - 57s 458ms/step - loss: 0.0683 - accuracy: 0.9777 - val_loss: 0.6621 - val_accuracy: 0.8705\n",
      "Epoch 73/100\n",
      "125/125 [==============================] - 71s 569ms/step - loss: 0.0673 - accuracy: 0.9780 - val_loss: 0.6653 - val_accuracy: 0.8702\n",
      "Epoch 74/100\n",
      "125/125 [==============================] - 55s 440ms/step - loss: 0.0656 - accuracy: 0.9785 - val_loss: 0.6703 - val_accuracy: 0.8712\n",
      "Epoch 75/100\n",
      "125/125 [==============================] - 52s 414ms/step - loss: 0.0647 - accuracy: 0.9786 - val_loss: 0.6695 - val_accuracy: 0.8711\n",
      "Epoch 76/100\n",
      "125/125 [==============================] - 52s 413ms/step - loss: 0.0633 - accuracy: 0.9790 - val_loss: 0.6808 - val_accuracy: 0.8701\n",
      "Epoch 77/100\n",
      "125/125 [==============================] - 53s 424ms/step - loss: 0.0621 - accuracy: 0.9795 - val_loss: 0.6851 - val_accuracy: 0.8702\n",
      "Epoch 78/100\n",
      "125/125 [==============================] - 76s 610ms/step - loss: 0.0611 - accuracy: 0.9795 - val_loss: 0.6886 - val_accuracy: 0.8694\n",
      "Epoch 79/100\n",
      "125/125 [==============================] - 68s 543ms/step - loss: 0.0601 - accuracy: 0.9798 - val_loss: 0.6885 - val_accuracy: 0.8694\n",
      "Epoch 80/100\n",
      "125/125 [==============================] - 54s 434ms/step - loss: 0.0590 - accuracy: 0.9804 - val_loss: 0.6927 - val_accuracy: 0.8699\n",
      "Epoch 81/100\n",
      "125/125 [==============================] - 51s 409ms/step - loss: 0.0579 - accuracy: 0.9806 - val_loss: 0.7000 - val_accuracy: 0.8697\n",
      "Epoch 82/100\n",
      "125/125 [==============================] - 54s 434ms/step - loss: 0.0562 - accuracy: 0.9812 - val_loss: 0.7092 - val_accuracy: 0.8689\n",
      "Epoch 83/100\n",
      "125/125 [==============================] - 51s 409ms/step - loss: 0.0558 - accuracy: 0.9810 - val_loss: 0.7052 - val_accuracy: 0.8701\n",
      "Epoch 84/100\n",
      "125/125 [==============================] - 54s 433ms/step - loss: 0.0549 - accuracy: 0.9815 - val_loss: 0.7120 - val_accuracy: 0.8697\n",
      "Epoch 85/100\n",
      "125/125 [==============================] - 53s 426ms/step - loss: 0.0541 - accuracy: 0.9817 - val_loss: 0.7154 - val_accuracy: 0.8695\n",
      "Epoch 86/100\n",
      "125/125 [==============================] - 58s 467ms/step - loss: 0.0531 - accuracy: 0.9818 - val_loss: 0.7199 - val_accuracy: 0.8682\n",
      "Epoch 87/100\n",
      "125/125 [==============================] - 84s 675ms/step - loss: 0.0518 - accuracy: 0.9823 - val_loss: 0.7209 - val_accuracy: 0.8699\n",
      "Epoch 88/100\n",
      "125/125 [==============================] - 53s 425ms/step - loss: 0.0512 - accuracy: 0.9825 - val_loss: 0.7319 - val_accuracy: 0.8693\n",
      "Epoch 89/100\n",
      "125/125 [==============================] - 53s 426ms/step - loss: 0.0507 - accuracy: 0.9826 - val_loss: 0.7314 - val_accuracy: 0.8687\n",
      "Epoch 90/100\n",
      "125/125 [==============================] - 53s 421ms/step - loss: 0.0499 - accuracy: 0.9828 - val_loss: 0.7272 - val_accuracy: 0.8699\n",
      "Epoch 91/100\n",
      "125/125 [==============================] - 53s 421ms/step - loss: 0.0489 - accuracy: 0.9829 - val_loss: 0.7337 - val_accuracy: 0.8687\n",
      "Epoch 92/100\n",
      "125/125 [==============================] - 52s 419ms/step - loss: 0.0486 - accuracy: 0.9833 - val_loss: 0.7396 - val_accuracy: 0.8694\n",
      "Epoch 93/100\n",
      "125/125 [==============================] - 53s 425ms/step - loss: 0.0475 - accuracy: 0.9836 - val_loss: 0.7395 - val_accuracy: 0.8688\n",
      "Epoch 94/100\n",
      "125/125 [==============================] - 53s 421ms/step - loss: 0.0472 - accuracy: 0.9835 - val_loss: 0.7501 - val_accuracy: 0.8691\n",
      "Epoch 95/100\n",
      "125/125 [==============================] - 53s 422ms/step - loss: 0.0465 - accuracy: 0.9838 - val_loss: 0.7451 - val_accuracy: 0.8691\n",
      "Epoch 96/100\n",
      "125/125 [==============================] - 53s 426ms/step - loss: 0.0460 - accuracy: 0.9839 - val_loss: 0.7505 - val_accuracy: 0.8686\n",
      "Epoch 97/100\n",
      "125/125 [==============================] - 57s 452ms/step - loss: 0.0450 - accuracy: 0.9841 - val_loss: 0.7541 - val_accuracy: 0.8688\n",
      "Epoch 98/100\n",
      "125/125 [==============================] - 55s 437ms/step - loss: 0.0444 - accuracy: 0.9844 - val_loss: 0.7550 - val_accuracy: 0.8688\n",
      "Epoch 99/100\n",
      "125/125 [==============================] - 54s 435ms/step - loss: 0.0440 - accuracy: 0.9845 - val_loss: 0.7620 - val_accuracy: 0.8689\n",
      "Epoch 100/100\n",
      "125/125 [==============================] - 52s 418ms/step - loss: 0.0433 - accuracy: 0.9848 - val_loss: 0.7606 - val_accuracy: 0.8686\n"
     ]
    }
   ],
   "source": [
    "'''Define the model that will tun'''\n",
    "# encoder_input_data & decoder_input_data into decoder_target_data\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "''' compiling the model '''\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "''' training '''\n",
    "history = model.fit([encoder_input_data, decoder_input_data], decoder_output_data,\n",
    "            batch_size=batch_size, epochs=epochs, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
